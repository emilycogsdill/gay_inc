{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# WHITELIST and RESERVED subdomains\n",
    "\n",
    "### Whitelist:\n",
    "\n",
    "1) most common first names, last names, and full names (of these, the MOST common first and last names will be designated \"premium\")\n",
    "\n",
    "2) common (but not \"reserved\") words and bigrams\n",
    "\n",
    "- top 100 single words and top 100 bigrams are reserved (200 total)\n",
    "- the next top 400 single words and bigrams each are whitelisted (800 total)\n",
    "\n",
    "### Reserved:\n",
    "\n",
    "(based on NYT data)\n",
    "\n",
    "- Top 100 single words\n",
    "\n",
    "- Top 100 bigrams\n",
    "\n",
    "- Others can be added/reclassified manually as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BORING\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top first names\n",
    "\n",
    "Source: https://raw.githubusercontent.com/hadley/data-baby-names/master/baby-names.csv\n",
    "\n",
    "Take baby names given to >0.05% of population (split by sex) in any year 1980 or later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the raw data\n",
    "#https://raw.githubusercontent.com/hadley/data-baby-names/master/baby-names.csv\n",
    "\n",
    "df = pd.read_csv('csvs/raw/baby-names.csv')\n",
    "\n",
    "\n",
    "#All first names make it into the whitelist\n",
    "\n",
    "whitelisted_names = [name.lower() for name in df['name'].unique()]\n",
    "\n",
    "#Premium are determined by all who were >0.1% of babies named in the year 1980 (partitioned by sex)\n",
    "#It's not perfect, but who among us can claim perfection?\n",
    "df_1980 = df[df['year']==1980][['name','percent','sex']]\n",
    "boy_names = df_1980[df_1980['sex']=='boy']\n",
    "girl_names = df_1980[df_1980['sex']=='girl']\n",
    "\n",
    "premium_boy_names = list(boy_names[boy_names['percent']>.001]['name'])\n",
    "premium_girl_names = list(girl_names[girl_names['percent']>.001]['name'])\n",
    "premium_first_names = [name.lower() for name in set(premium_boy_names + premium_girl_names)]\n",
    "normie_first_names = [name for name in whitelisted_names if name not in premium_first_names]\n",
    "\n",
    "#twerk some dataframes to combine, separating out Premium vs. Normie tier first names\n",
    "df_premium = pd.DataFrame(premium_first_names,columns=['name'])\n",
    "df_premium['level']='premium'\n",
    "\n",
    "df_normie = pd.DataFrame(normie_first_names,columns=['name'])\n",
    "df_normie['level']='normie'\n",
    "\n",
    "first_names_df = df_premium.merge(df_normie,how='outer')\n",
    "first_names_df['category']='whitelist'\n",
    "first_names_df['name_type']='first'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top last names\n",
    "\n",
    "Top 1000 surnames according to https://www.thoughtco.com/most-common-us-surnames-1422656\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#load the raw data\n",
    "#https://raw.githubusercontent.com/hadley/data-baby-names/master/baby-names.csv\n",
    "\n",
    "df = pd.read_csv('csvs/raw/top_1000_surnames.csv')\n",
    "\n",
    "#oops there are some janky rows in there\n",
    "df = df[~df['count'].isna()]\n",
    "\n",
    "#Premium: top 100 surnames\n",
    "#Normie: all other top 1000\n",
    "df['level'] = ''\n",
    "df[0:100]['level'] = 'premium'\n",
    "df[101:]['level'] = 'normie'\n",
    "df['name'] = [x.lower() for x in df['name']]\n",
    "df['category'] = 'whitelist'\n",
    "df['name_type'] = 'last'\n",
    "\n",
    "surnames_df = df[['name','category','level','name_type']]\n",
    "\n",
    "#combine firstname and lastname dataframes\n",
    "top_names_df = surnames_df.merge(first_names_df,how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Full Names\n",
    "- combine premium first names + premium last names and add those full names to the whitelist\n",
    "- it's 299 premium first names * 100 premium surnames = 29900 rows, I think the database can handle it lol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_premium_names = top_names_df[top_names_df['level']=='premium']\n",
    "first_names = list(df_premium_names[df_premium_names['name_type']=='first']['name'])\n",
    "last_names = list(df_premium_names[df_premium_names['name_type']=='last']['name'])\n",
    "\n",
    "full_names_list=[]\n",
    "for first_name in first_names:\n",
    "    for last_name in last_names:\n",
    "        full_name = first_name+last_name\n",
    "        full_names_list.append(full_name)\n",
    "        \n",
    "full_names_df = pd.DataFrame(full_names_list, columns=['name'])        \n",
    "full_names_df['category']='whitelist'\n",
    "full_names_df['level']='normie'\n",
    "full_names_df['name_type']='full'\n",
    "\n",
    "#combine first names, last names, and full names into a single dataframe\n",
    "top_names_df = top_names_df.merge(full_names_df,how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Words and Ngrams\n",
    "- based on NYT data\n",
    "- some of these will be set aside as \"reserved\" (TBD)\n",
    "\n",
    "## Reserved subdomains\n",
    "- top 100 words\n",
    "- top 100 bigrams\n",
    "\n",
    "## Whitelisted, available common words\n",
    "- top 400 non-reserved single words and bigrams \n",
    "- (800 total; just the next 400 in the top 500 of each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_top_words_and_bigrams.py\n",
    "\n",
    "#source data: https://www.cs.utexas.edu/~gdurrett/courses/fa2020/nyt.txt\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#read in text data\n",
    "string = open('csvs/raw/nyt.txt', 'r').read().lower() #from https://www.cs.utexas.edu/~gdurrett/courses/fa2020/nyt.txt\n",
    "\n",
    "#tokenize it (this flavor ignores punctuation)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(string)\n",
    "\n",
    "#get list of single word frequencies\n",
    "unifreq = nltk.FreqDist(tokens)\n",
    "\n",
    "#get list of bigrams from the tokens list\n",
    "bigrams_list = list(nltk.bigrams(tokens))\n",
    "bifreq = nltk.FreqDist(bigrams_list)\n",
    "\n",
    "#Top 500 individual words (excluding numbers and single-word tokens)\n",
    "top_words = unifreq.most_common(1000)\n",
    "rank=1\n",
    "unigram_list = []\n",
    "unigram_ranks = []\n",
    "for word in top_words:\n",
    "    if rank<=500:\n",
    "        word = word[0]\n",
    "        if len(word)>1 and not word.isnumeric():\n",
    "            unigram_list.append(word)\n",
    "            unigram_ranks.append(rank)\n",
    "            rank+=1\n",
    "\n",
    "#Top 500 bigrams        \n",
    "top_bigrams = bifreq.most_common(500)\n",
    "rank=1\n",
    "bigram_list = []\n",
    "bigram_ranks = []\n",
    "for bigram in top_bigrams:\n",
    "    bigram = bigram[0][0]+bigram[0][1]\n",
    "    bigram_list.append(bigram)\n",
    "    bigram_ranks.append(rank)\n",
    "    rank+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "top_words_df = pd.DataFrame(zip(unigram_list,unigram_ranks),columns=['name','rank'])\n",
    "top_words_df['category']='sup'\n",
    "top_words_df[0:99]['category']='reserved'\n",
    "top_words_df[100:]['category']='whitelist'\n",
    "\n",
    "top_bigrams_df = pd.DataFrame(zip(bigram_list,bigram_ranks),columns=['name','rank'])\n",
    "top_bigrams_df['category']='sup'\n",
    "top_bigrams_df[0:99]['category']='reserved'\n",
    "top_bigrams_df[100:]['category']='whitelist'\n",
    "\n",
    "#set up the final \"top common words and bigrams\" dataframe\n",
    "combined_words_df = top_words_df.merge(top_bigrams_df,how='outer')\n",
    "combined_words_df['level']='normie'\n",
    "combined_words_df['name_type']='words'\n",
    "top_words_df_final = combined_words_df[['name','category','level','name_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the top names and top words dataframes. This is our final whitelist YAY\n",
    "whitelist_and_reserved_df = combined_words_df.merge(top_names_df,how='outer')\n",
    "whitelist_and_reserved_df = whitelist_and_reserved_df[['name','category','level']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write our whitelist and reserved CSV\n",
    "whitelist_and_reserved_df.to_csv('csvs/whitelist_and_reserved.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding CSV to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(whitelist_and_reserved_df)):\n",
    "\n",
    "all_row_strings = []\n",
    "for i in range(len(whitelist_and_reserved_df)):\n",
    "    name = whitelist_and_reserved_df.loc[i]['name']\n",
    "    category = whitelist_and_reserved_df.loc[i]['category']\n",
    "    level = whitelist_and_reserved_df.loc[i]['level']\n",
    "    row_string = \"('\"+name+\"','\"+category+\"','\"+level+\"')\"\n",
    "    all_row_strings.append(row_string)\n",
    "    \n",
    "full_insert_string = ','.join(all_row_strings)\n",
    "sql = f'''INSERT INTO whitelist (name,category,level) VALUES {full_insert_string};\n",
    "'''\n",
    "\n",
    "print(sql)\n",
    "# why yes I DID just copy-paste this into my terminal to run it locally.\n",
    "# Pandas is scuffed in Cpanel (deeply mysterious)\n",
    "# but this isn't huge data anyway, it's only like ~38k rows\n",
    "# and I have other things I need to do"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
